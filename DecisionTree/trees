# -*- coding=utf-8 -*-
from math import log

import operator


class DecisionTree:


    '''
    决策树是以特征划分数据集，知道自己中的每一个子项都是同一类别之后，训练完成，并将类别作为标签返回

    数据集划分原则：将无需数据划分得更加有序。
    组织杂乱无章的数据的方法：1、信息论量化度量 2、基尼不纯度

    信息论量化方式：
    划分数据集前后信息发生的变化称为信息增益，获得信息增益最高的特征就是最好的划分依据

    信息增益的度量方式称为“熵”，熵定义为信息的期望值
    信息：如果待分类的实物可能划分在多个分类之中，则符号x的信息定义为：l(x) = log2(p(x)),其中p(x)是选择该分类的概率
    熵： H = -p(x1)log(p(x1)) - p(x2)log(p(x)) - .....

    计算数据集熵
    '''
    def calcShannonEnt(self, dataSet):
        numOfEntities = len(dataSet)
        labelCounts = {}
        for featVec in dataSet:
            #取最后一列的当前特征值
            featLabel = featVec[-1]
            '''
            记录当前特征值出现的次数
            如果当前特征值不在记录字典中，则添加
            '''
            if featLabel not in labelCounts.keys():
                labelCounts[featLabel] = 0
            labelCounts[featLabel] += 1
        shannonEnt = 0.0

        '''
        以类别出现的概率作为分类选择概率
        '''
        for key in labelCounts.keys():
            #计算
            prob = float(labelCounts[key])/numOfEntities
            shannonEnt -= prob * log(prob, 2)
        return shannonEnt

    def createSampleData(self):
        dataSet = [
            [1, 1, 'yes'],
            [1, 0, 'no'],
            [1, 1, 'yes'],
            [0, 1, 'no'],
            [0, 1, 'no']
        ]

        labels = ['no surfacing', 'flippers']
        return dataSet, labels

    '''
    用于按照给定的特征划分数据集
    ‘axis’表示划分的特征
    ‘value’表示该特征应取的值
    '''
    def splitDataSet(self, dataSet, axis, value):
        retDataSet = []
        for featureVec in dataSet:
            if featureVec[axis] == value:
                splitDataVecTmp = featureVec[:axis]
                splitDataVecTmp.extend(featureVec[axis+1 :])
                retDataSet.append(splitDataVecTmp)
        return retDataSet


    def chooseBestFeatureToSplit(self,dataSet):
        #最后一行最为分类值，所以除开最后一列为特征数量
        numFeatures = len(dataSet[0]) - 1
        baseEntropy = self.calcShannonEnt(dataSet)
        bastInfoGain = 0.0
        bestFea = -1
        for i in range(numFeatures):
            #获取每一列的特征值
            feaList = [example[i] for example in dataSet]
            #创建唯一标签分类列表
            uniqueVals = set(feaList)
            newEntory = 0.0
            #以特征值作为数据集划分依据
            for value in uniqueVals:
                subDataSet = self.splitDataSet(dataSet,axis=i,value=value)
                prob = len(subDataSet)/float(len(dataSet))
                newEntory += prob * self.calcShannonEnt(subDataSet)
            infoGain = baseEntropy - newEntory

            #保留最好的信息增益
            if infoGain > bastInfoGain :
                bastInfoGain = infoGain
                bestFea = i
        return bestFea

    '''
    返回出现次数最多的分类名称
    '''
    def majorityCnt(self,classList):
        classCount = {}
        for vote in classList:
            if vote not in classCount.keys():classCount[vote] = 0
            classCount[vote] += 1
        sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)
        return sortedClassCount[0][0]


    '''
    划分数据集，并对划分出来的每一个数据集在进行划分，直至程序遍历完所有的数据集或者每个分支下的所有实例都具有相同的分类
    所有实例具有相同的分类，则得到一个叶子节点或者终止块
    '''
    def createTree(self, dataSet, labels):
        classList = [example[-1] for example in dataSet]
        #当类别完全相同是，则停止划分
        if classList.count(classList[0]) == len(classList):
            return classList[0]
        #遍历完所有的特征时返回出现次数最多的类别
        '''
        出现该情况的原因是函数使用完了所有的特征，依然不能够使得子集内的所有数据都是同一个类别，
        那么就选择一个出现次数最多的类别作为标签
        '''
        if len(dataSet[0]) == 1:
            return self.majorityCnt(classList)
        bestFea = self.chooseBestFeatureToSplit(dataSet)
        bestFeaLabel = labels[bestFea]
        myTree = {bestFeaLabel:{}}
        del (labels[bestFea])
        #得出列表包含的所有属性值
        featValues = [example[bestFea] for example in dataSet]
        uniqueVals = set(featValues)
        #根据特征值来对每一个子树进行划分
        for valus in uniqueVals:
            subLables = labels[:]
            myTree[bestFeaLabel][valus] = self.createTree(self.splitDataSet(dataSet, bestFea, valus), subLables)
        return myTree


if __name__ == '__main__':
    mat,labels = DecisionTree().createSampleData()
    mytree = DecisionTree().createTree(mat,labels)
    print mytree

