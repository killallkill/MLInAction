# -*- coding=utf-8 -*-
from math import log

class DecisionTree:


    '''
    决策树是以特征划分数据集，知道自己中的每一个子项都是同一类别之后，训练完成，并将类别作为标签返回

    数据集划分原则：将无需数据划分得更加有序。
    组织杂乱无章的数据的方法：1、信息论量化度量 2、基尼不纯度

    信息论量化方式：
    划分数据集前后信息发生的变化称为信息增益，获得信息增益最高的特征就是最好的划分依据

    信息增益的度量方式称为“熵”，熵定义为信息的期望值
    信息：如果待分类的实物可能划分在多个分类之中，则符号x的信息定义为：l(x) = log2(p(x)),其中p(x)是选择该分类的概率
    熵： H = -p(x1)log(p(x1)) - p(x2)log(p(x)) - .....

    计算数据集熵
    '''
    def calcShannonEnt(self, dataSet):
        numOfEntities = len(dataSet)
        labelCounts = {}
        for featVec in dataSet:
            #取最后一列的当前特征值
            featLabel = featVec[-1]
            '''
            记录当前特征值出现的次数
            如果当前特征值不在记录字典中，则添加
            '''
            if featLabel not in labelCounts.keys():
                labelCounts[featLabel] = 0
            labelCounts[featLabel] += 1
        shannonEnt = 0.0

        '''
        以类别出现的概率作为分类选择概率
        '''
        for key in labelCounts.keys():
            #计算
            prob = float(labelCounts[key])/numOfEntities
            shannonEnt -= prob * log(prob, 2)
        return shannonEnt

    def createSampleData(self):
        dataSet = [
            [1, 1, 'yes'],
            [1, 0, 'yes'],
            [1, 1, 'yes'],
            [0, 1, 'no'],
            [0, 1, 'no']
        ]

        labels = ['no surfacing', 'flippers']
        return dataSet, labels

    '''
    用于按照给定的特征划分数据集
    ‘axis’表示划分的特征
    ‘value’表示该特征应取的值
    '''
    def splitDataSet(self, dataSet, axis, value):
        retDataSet = []
        for featureVec in dataSet:
            if featureVec[axis] == value:
                splitDataVecTmp = featureVec[:axis]
                splitDataVecTmp.extend(featureVec[axis+1 :])
                retDataSet.append(splitDataVecTmp)
        return retDataSet


    def chooseBestFeatureToSplit(self,dataSet):
        #最后一行最为分类值，所以除开最后一列为特征数量
        numFeatures = len(dataSet[0]) - 1
        baseEntropy = self.calcShannonEnt(dataSet)
        bastInfoGain = 0.0
        bestFea = -1
        for i in range(numFeatures):
            #获取每一列的特征值
            feaList = [example[i] for example in dataSet]
            #创建唯一标签分类列表
            uniqueVals = set(feaList)
            newEntory = 0.0
            #以特征值作为数据集划分依据
            for value in uniqueVals:
                subDataSet = self.splitDataSet(dataSet,axis=i,value=value)
                prob = len(subDataSet)/float(len(dataSet))
                newEntory += prob * self.calcShannonEnt(subDataSet)
            infoGain = baseEntropy - newEntory

            #保留最好的信息增益
            if infoGain > bastInfoGain :
                bastInfoGain = infoGain
                bestFea = i
        return bestFea



if __name__ == '__main__':
    mat,labels = DecisionTree().createSampleData()
    feature = DecisionTree().chooseBestFeatureToSplit(mat)
    print feature

